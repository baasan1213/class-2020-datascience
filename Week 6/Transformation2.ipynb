{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"transformation 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy(f, numPartitions=None, partitionFunc=\"function portable_hash\")\n",
    "\n",
    "Return an RDD of grouped items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize ([1, 1, 2, 3, 4, 5, 5, 6, 6, 6])\n",
    "result = data.groupBy(lambda x: x%3).mapValues(list).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey(numPartitions=None, partitionFunc=\"function portable_hash\")\n",
    "\n",
    "Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.\n",
    "\n",
    "Note If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 3)])\n",
    "print(data.groupByKey().mapValues(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey vs. ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(10)).map(lambda a: (a%3, a))\n",
    "data.reduceByKey(lambda a, b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupByKey().map(lambda t: (t[0], sum(t[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey(ascending=True, numPartitions=None, keyfunc=\"lambda function\")\n",
    "\n",
    "Sorts this RDD, which is assumed to consist of (key, value) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([(\"b\", 1), (\"a\", 1), (\"c\", 1), (\"a\", 1)])\n",
    "data.sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortBy(keyfunc, ascending=True, numPartitions=None)\n",
    "Sorts this RDD by the given keyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([4, 3, 2, 4, 2, 7, 9, 4, 5, 2])\n",
    "data.sortBy(lambda x: x, False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce(numPartitions, shuffle=False)\n",
    "Return a new RDD that is reduced into numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(15), 4)\n",
    "data.coalesce(10).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition(numPartitions)\n",
    "Return a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. ***If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample(withReplacement, fraction, seed=None)\n",
    "Return a sampled subset of this RDD.\n",
    "\n",
    "Parameters\n",
    "* withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "* fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be \\[0, 1\\] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "* seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(True, 0.5, 13).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct(numPartitions=None)\n",
    "Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([1, 1, 2, 3, 4, 5, 6, 2, 3, 5])\n",
    "data.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
