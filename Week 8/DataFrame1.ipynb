{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n",
    "\n",
    "# get context from the session\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "***Creates a DataFrame from an RDD, a list*** or a pandas.DataFrame.\n",
    "\n",
    "When schema is a list of column names, the type of each column will be inferred from data.\n",
    "\n",
    "When schema is None, *it will try to infer the schema (column names and types) from data*, which should be an RDD of either Row, namedtuple, or dict.\n",
    "\n",
    "When schema is pyspark.sql.types.DataType or a datatype string, it must match the real data, or an exception will be thrown at runtime. If the given schema is not pyspark.sql.types.StructType, it will be wrapped into a pyspark.sql.types.StructType as its only field, and the field name will be “value”. Each record will also be wrapped into a tuple, which can be converted to row later.\n",
    "\n",
    "If schema inference is needed, samplingRatio is used to determined the ratio of rows used for schema inference. The first row will be used if samplingRatio is None.\n",
    "\n",
    "Parameters\n",
    "* data – an RDD of any kind of SQL data representation (e.g. row, tuple, int, boolean, etc.), list, or pandas.DataFrame.\n",
    "* schema – a pyspark.sql.types.DataType or a datatype string or a list of column names, default is None.\n",
    "* samplingRatio – the sample ratio of rows used for inferring\n",
    "* verifySchema – verify data types of every row against schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json(path, schema=None, ...)\n",
    "\n",
    "Loads JSON files and returns the results as a DataFrame.\n",
    "\n",
    "JSON Lines (newline-delimited JSON) is supported by default. For JSON (one record per file), set the multiLine parameter to true.\n",
    "\n",
    "If the schema parameter is not specified, this function goes through the input once to determine the input schema.\n",
    "\n",
    "Parameters\n",
    "* path – string represents path to the JSON dataset, or a list of paths, or RDD of Strings storing JSON objects.\n",
    "* schema – an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createOrReplaceTempView(name)\n",
    "Creates or replaces a local temporary view with this DataFrame.\n",
    "\n",
    "***It creates (or replaces if that view name already exists) a lazily evaluated \"view\" that you can then use like a hive table in Spark SQL.***\n",
    "\n",
    "It does not persist to memory unless you cache the dataset that underpins the view.\n",
    "The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select(*cols)\n",
    "Projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "Parameters\n",
    "* cols – list of column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selectExpr(*expr)\n",
    "Projects a set of SQL expressions and returns a new DataFrame.\n",
    "\n",
    "This is a variant of select() that accepts SQL expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
